from products.services.vectorizer import vectorize_product, vectorize_user
from products.services.cluster_distribution import get_top_products_by_cluster
from products.services.math_utils import calculate_confidence, cosine_similarity, cluster_weight
from ai.services.recommendation_explainer import explain_recommendation, get_embedding

from products.models import ProductOption, RecommendationHistory

from concurrent.futures import ThreadPoolExecutor # ë³‘ë ¬ ì²˜ë¦¬ (ê³„ì‚° ë¹ ë¥´ê²Œ)

def recommend_products(user, top_n=3, user_query=None):
    profile = user.financialprofile
    user_vec = vectorize_user(profile)
    # 1. ì‚¬ìš©ì ì¿¼ë¦¬ ì„ë² ë”© (ê²€ìƒ‰ì–´ê°€ ìˆì„ ë•Œë§Œ)
    query_vec = get_embedding(user_query) if user_query else None

    cluster_products = get_top_products_by_cluster(profile.cluster_label)
    cluster_prob_map = {
        p['product_option_id']: p['ratio']
        for p in cluster_products
    }


    scored = []
    # 1ë‹¨ê³„: ëª¨ë“  ìƒí’ˆì— ëŒ€í•´ ë§ˆì´ë°ì´í„° ìœ ì‚¬ë„(Base Score)ë§Œ ê³„ì‚° (API í˜¸ì¶œ X)
    for option in ProductOption.objects.all():
        product_vec = vectorize_product(option)
        sim = cosine_similarity(user_vec, product_vec)
        weight = cluster_weight(cluster_prob_map.get(option.id, 0.01))
        
        scored.append({
            'product_option': option,
            'base_score': sim * weight,
            'similarity': sim,
        })

    # 2ë‹¨ê³„: ë§ˆì´ë°ì´í„° ì ìˆ˜ ë†’ì€ ìƒìœ„ 20ê°œë§Œ í›„ë³´(Candidates)ë¡œ ì„ ì •
    candidates = sorted(scored, key=lambda x: x['base_score'], reverse=True)[:20]

    # 3ë‹¨ê³„: í›„ë³´ 20ê°œì— ëŒ€í•´ì„œë§Œ ìì—°ì–´ ìœ ì‚¬ë„(Semantic Score) ê³„ì‚°
    query_vec = get_embedding(user_query) if user_query else None
    all_final_scores = []

    for item in candidates:
        final_score = item['base_score']
        
        if query_vec:
            # 2. ğŸ”¥ [í•µì‹¬] 20ê°œì˜ ì„ë² ë”©ì„ 'ë™ì‹œì—' ìš”ì²­ (ë³‘ë ¬ ì²˜ë¦¬)
            def fetch_semantic_score(item):
                opt = item['product_option']
                text = f"{opt.product.fin_prdt_nm} {opt.product.etc_note}"
                prod_emb = get_embedding(text)
                if prod_emb:
                    sem_sim = cosine_similarity(query_vec, prod_emb)
                    item['score'] = (item['base_score'] * 0.7) + (sem_sim * 0.3)
                else:
                    item['score'] = item['base_score']
                return item

            # ìµœëŒ€ 20ê°œì˜ ì“°ë ˆë“œë¥¼ ì—´ì–´ í•œêº¼ë²ˆì— API ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤.
            with ThreadPoolExecutor(max_workers=20) as executor:
                candidates = list(executor.map(fetch_semantic_score, candidates))
        else:
            for item in candidates:
                item['score'] = item['base_score']

        # ğŸ”¥ [í•µì‹¬ ìˆ˜ì •] confidence ê³„ì‚°ì„ ìœ„í•´ ëª¨ë“  í›„ë³´ì˜ score ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        all_final_scores = [c['score'] for c in candidates]
        
        # 2. ê° candidateì— confidence í‚¤ ì¶”ê°€
        for c in candidates:
            # calculate_confidence í•¨ìˆ˜ê°€ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
            c["confidence"] = calculate_confidence(c["score"], all_final_scores)

        # 3. ìµœì¢… ì •ë ¬ ë° ìƒìœ„ Nê°œ ì¶”ì¶œ (ì´ì œ confidenceê°€ í¬í•¨ë¨)
        top_recommendations = sorted(candidates, key=lambda x: x['score'], reverse=True)[:top_n]

        # 4. AI ë¶„ì„ ë³‘ë ¬ í˜¸ì¶œ
        with ThreadPoolExecutor(max_workers=top_n) as executor:
            reasons = list(executor.map(lambda r: explain_recommendation(user, r, user_query), top_recommendations))

        for i, r in enumerate(top_recommendations):
            r["ai_analysis"] = reasons[i]

        return top_recommendations

    # all_scores = []

    # # ìµœì í™”: ì¿¼ë¦¬ë§ˆë‹¤ DB ì ‘ê·¼ì„ ì¤„ì´ê¸° ìœ„í•´ select_related ì‚¬ìš© ê¶Œì¥
    # options = ProductOption.objects.select_related('product').all()

    # for option in options:
    #     # 1. ë§ˆì´ë°ì´í„° ê¸°ë°˜ ê¸°ë³¸ ì ìˆ˜
    #     product_vec = vectorize_product(option)
    #     sim = cosine_similarity(user_vec, product_vec)

    #     cluster_prob = cluster_prob_map.get(option.id, 0.01)
    #     weight = cluster_weight(cluster_prob)

    #     # ê¸°ì´ˆ ì ìˆ˜
    #     final_score = sim * weight
            
    #     # 2. [í•µì‹¬] ì„ë² ë”© ê¸°ë°˜ ì‹œë§¨í‹± ìœ ì‚¬ë„ ê°€ì‚°ì 
    #     if query_vec:
    #         # ìƒí’ˆëª…ê³¼ ì„¤ëª…ì„ í•©ì³ì„œ ì„ë² ë”© (ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„  ì´ ê°’ì„ DBì— ë¯¸ë¦¬ ì €ì¥í•´ë‘ëŠ”ê²Œ ë² ìŠ¤íŠ¸!)
    #         product_text = f"{option.product.fin_prdt_nm} {option.product.etc_note}"
    #         product_text_vec = get_embedding(product_text)
            
    #         if product_text_vec:
    #             semantic_sim = cosine_similarity(query_vec, product_text_vec)
    #             # ë§ˆì´ë°ì´í„° ì ìˆ˜ 70% + ìì—°ì–´ ê²€ìƒ‰ ì ìˆ˜ 30% ë¹„ìœ¨ë¡œ í˜¼í•©
    #             final_score = (final_score * 0.7) + (semantic_sim * 0.3)

    #     all_scores.append(final_score)

    #     scored.append({
    #         'product_option': option,
    #         'score': final_score,
    #         'similarity': sim,
    #         'cluster_weight': weight,
    #         # gms explainerì— í•„ìš”í•œ ë°ì´í„° ë¯¸ë¦¬ ë§¤ì¹­
    #         'fin_prdt_nm': option.product.fin_prdt_nm,
    #         'intr_rate': option.intr_rate,
    #         'intr_rate2': option.intr_rate2,
    #         'save_trm': option.save_trm,
    #     })

    # for r in scored:
    #     r["confidence"] = calculate_confidence(r["score"], all_scores)
        
    # ğŸ”¥ 3ë‹¨ê³„: ë¨¼ì € "ì •ë ¬"í•˜ê³  "ìë¥´ê¸°" (ì—¬ê¸°ê°€ í¬ì¸íŠ¸!)
    # top_recommendations = sorted(scored, key=lambda x: x['score'], reverse=True)[:top_n]
        
    # ğŸ”¥ 4. AI ì„¤ëª… ë³‘ë ¬ í˜¸ì¶œ (ì†ë„ í–¥ìƒì˜ í•µì‹¬!)
    # max_workersëŠ” ë™ì‹œì— ë³´ë‚¼ ìš”ì²­ ìˆ˜ì…ë‹ˆë‹¤.
    # with ThreadPoolExecutor(max_workers=top_n) as executor:
    #     # ê° ì¶”ì²œ ìƒí’ˆì— ëŒ€í•´ explain_recommendation í•¨ìˆ˜ë¥¼ ë™ì‹œì— ì‹¤í–‰
    #     reasons = list(executor.map(lambda r: explain_recommendation(user, r, user_query), top_recommendations))

    # # 5. ìƒì„±ëœ ì„¤ëª…ì„ ê²°ê³¼ì— ë§¤ì¹­
    # # ì´ì œ reasonsëŠ” JSON(dict) í˜•íƒœ
    # for i, r in enumerate(top_recommendations):
    #     # ì´ì œ r["reason"]ì—ëŠ” dict{"reason", "report", "nudge"} ì „ì²´ê°€ ë“¤ì–´ê°‘ë‹ˆë‹¤.
    #     r["ai_analysis"] = reasons[i]

    # return top_recommendations


# ì¶”ì²œ ìƒí’ˆ ê¸°ë¡í•˜ê¸° (db)
def save_recommendations(user, profile, recommendations):
    histories = []

    for rec in recommendations:
        histories.append(
            RecommendationHistory(
                user=user,
                product_option=rec["product_option"],
                score=rec["score"],
                confidence=rec["confidence"],
                cluster_label=profile.cluster_label,
            )
        )

    RecommendationHistory.objects.bulk_create(histories)
    return histories
